{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = {'svg', 'png'}[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, copy, time, pickle\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "\n",
    "import torch\n",
    "from torch.distributions import Categorical, Normal\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "\n",
    "\n",
    "from reinforce_agent import *\n",
    "from surrogate import *\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True, floatmode='fixed', linewidth=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropInteDiff(object):\n",
    "    def __init__(self, kp, ki, kd, limits_out_all=[None, None]):\n",
    "        self.kp = kp\n",
    "        self.ki = ki\n",
    "        self.kd = kd\n",
    "        self.limits_out_all = limits_out_all\n",
    "\n",
    "        # internals\n",
    "        self.err_cur = 0\n",
    "        self.err_last = None\n",
    "        self.err_sum = 0\n",
    "\n",
    "        self.ep = 0\n",
    "        self.ei = 0\n",
    "        self.ed = 0\n",
    "\n",
    "        self.out_p = 0\n",
    "        self.out_i = 0\n",
    "        self.out_d = 0\n",
    "        self.out_all = 0\n",
    "\n",
    "    def step(self, err, dt):\n",
    "        '''The current error should be the expected value minus the current value.\n",
    "        '''\n",
    "        self.err_cur = err\n",
    "        self.err_sum = self.err_sum + self.err_cur\n",
    "\n",
    "        self.ep = self.err_cur\n",
    "        self.ei = self.err_sum\n",
    "        self.ed = self.err_cur - (self.err_last if (self.err_last is not None) else self.err_cur)\n",
    "\n",
    "        self.out_p = self.kp * self.ep\n",
    "        self.out_i = self.ki * self.ei * dt\n",
    "        self.out_i = np.clip(self.out_i, *self.limits_out_all)\n",
    "        self.out_d = self.kd * self.ed / dt\n",
    "\n",
    "        self.out_all = self.out_p + self.out_i + self.out_d\n",
    "        self.out_all = np.clip(self.out_all, *self.limits_out_all)\n",
    "        \n",
    "        self.err_last = self.err_cur\n",
    "        return self.out_all\n",
    "\n",
    "    def reset(self):\n",
    "        self.err_cur = 0\n",
    "        self.err_last = None\n",
    "        self.err_sum = 0\n",
    "\n",
    "        self.ep = 0\n",
    "        self.ei = 0\n",
    "        self.ed = 0\n",
    "\n",
    "        self.out_p = 0\n",
    "        self.out_i = 0\n",
    "        self.out_d = 0\n",
    "        self.out_all = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def homogeneous_rotation_matrix(theta) -> np.ndarray:\n",
    "    rotation_matrix = np.array([\n",
    "        [np.cos(theta), -np.sin(theta), 0],\n",
    "        [np.sin(theta), np.cos(theta), 0],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    return rotation_matrix\n",
    "\n",
    "\n",
    "def homogeneous_translation_matrix(translation) -> np.ndarray:\n",
    "    translation_matrix = np.array([\n",
    "        [1, 0, translation[0]],\n",
    "        [0, 1, translation[1]],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    return translation_matrix\n",
    "\n",
    "\n",
    "def homogeneous_transformation_matrix(translation, theta):\n",
    "    translation_matrix = homogeneous_translation_matrix(translation)\n",
    "    rotation_matrix = homogeneous_rotation_matrix(theta)\n",
    "    transformation_matrix = np.dot(translation_matrix, rotation_matrix)\n",
    "    return transformation_matrix\n",
    "\n",
    "\n",
    "def decompose_transformation_matrix(T):\n",
    "    translation = T[:2, 2]\n",
    "    rotation = T[:2, :2]\n",
    "    return translation, rotation\n",
    "\n",
    "\n",
    "def extract_theta_from_rotation(rotation_matrix):\n",
    "    theta = np.arctan2(rotation_matrix[1, 0], rotation_matrix[0, 0])\n",
    "    return theta\n",
    "\n",
    "\n",
    "def coordinate_transfer(tf, pose2d):    \n",
    "    t, r = decompose_transformation_matrix(np.dot(tf, homogeneous_transformation_matrix(pose2d[:-1], pose2d[-1])))\n",
    "    return np.array([t[0], t[1], extract_theta_from_rotation(r)])\n",
    "\n",
    "\n",
    "def mapping_period(ori_angle, angle_range) -> np.ndarray:\n",
    "    lower, upper = angle_range\n",
    "    period = upper - lower\n",
    "    res_angle = ori_angle % period\n",
    "    if res_angle > upper:\n",
    "        res_angle = res_angle - period\n",
    "    return res_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller():    \n",
    "    def __init__(self, name:str) -> None:\n",
    "        self.name = name\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        self.time = 0\n",
    "    \n",
    "    def get_action(self, obs:np.ndarray, dt:float, **kwargs) -> np.ndarray:\n",
    "        self.time += dt\n",
    "    \n",
    "\n",
    "class DiscreteDrlController(Controller):\n",
    "    def __init__(self, name, pi_wb_path, obs_dims, act_nums, hidden_size, device:str) -> None:\n",
    "        super().__init__(name)\n",
    "        self.device = torch.device(device)\n",
    "        self.pi_net = DiscretePolicyNetwork(obs_dims, act_nums, hidden_size).to(self.device)\n",
    "        \n",
    "        assert os.path.exists(pi_wb_path), f\"Path '{pi_wb_path}' of weights and biases is NOT exist.\"\n",
    "        self.pi_net.load_state_dict(torch.load(pi_wb_path))\n",
    "        self.pi_net.eval()\n",
    "    \n",
    "    def get_action(self, obs:np.ndarray, dt:float, deterministic=True) -> np.ndarray:\n",
    "        super().get_action(obs, dt)\n",
    "        prob = self.pi_net(torch.FloatTensor(obs).to(self.device))\n",
    "        if deterministic:\n",
    "            act = torch.argmax(prob, dim=0)\n",
    "        else:\n",
    "            act = Categorical(prob).sample()\n",
    "        return act.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_format(ax, title, xlabel, ylabel, xlim=None, ylim=None):\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    \n",
    "    if xlim is not None:\n",
    "        ax.set_xlim(*xlim)\n",
    "    if ylim is not None:\n",
    "        ax.set_ylim(*ylim)\n",
    "    \n",
    "    ax.set_aspect(1)\n",
    "\n",
    "    ax.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "    ax.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "\n",
    "    ax.tick_params(axis='both', which='both', direction='in', top=True, right=True)\n",
    "\n",
    "    # ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "def print_pose(ax, pose, l, c, label):\n",
    "    ax.plot(pose[0], pose[1], marker='.', color=c, label=label)\n",
    "    ax.plot([pose[0], pose[0] + np.cos(pose[2]) * l], [pose[1], pose[1] + np.sin(pose[2]) * l], color=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_episode_rollout(env:gym.Env, controller:Controller, deterministic=True, verbose=1):\n",
    "    log = []\n",
    "    \n",
    "    if controller is not None:\n",
    "        controller.reset()\n",
    "    \n",
    "    obs, info = env.reset()\n",
    "    while True:\n",
    "        if controller is not None:\n",
    "            act = controller.get_action(obs, env.dt, deterministic=deterministic)\n",
    "        else:\n",
    "            act = env.action_space.sample()\n",
    "        \n",
    "        next_obs, rew, ter, tru, info = env.step(act)\n",
    "        \n",
    "        if verbose >= 2:\n",
    "            print(obs, act, next_obs, rew, ter, tru, info)\n",
    "        \n",
    "        log.append({\n",
    "            'state':copy.deepcopy(env.state),\n",
    "            'obs':obs,\n",
    "            'act':act,\n",
    "            'next_obs':next_obs,\n",
    "            'rew':rew,\n",
    "        })\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "        if ter or tru:\n",
    "            break\n",
    "\n",
    "    log = {key: np.array([d[key] for d in log]) for key in log[0]}\n",
    "    \n",
    "    if verbose >= 1:\n",
    "        print(f\"The episodic length is: {len(log['rew'])} and episodic return is: {log['rew'].sum()}.\")\n",
    "    \n",
    "    return log, log['rew'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position Control Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionControlController(Controller):\n",
    "    def __init__(self, name:str, kp:float, ki:float, kd:float) -> None:\n",
    "        super().__init__(name)\n",
    "        self.pid = PropInteDiff(kp, ki, kd, limits_out_all=[-0.5, 0.5])\n",
    "\n",
    "        self.D = np.array([[0.14, 0.13], [0.15, 0.16], [0.15, 0.15], [0.15, 0.17], [0.15, 0.16], [0.14, 0.16], [0.16, 0.16]])\n",
    "        self.A = np.array([[-0.26, -0.41], [-0.10, -0.24], [-0.06, -0.15], [0.01, -0.04], [0.11, 0.11], [0.20, 0.13], [0.24, 0.38]])\n",
    "        self.X_m = np.array([[-0.26, -0.26], [-0.18, -0.18], [-0.09, -0.09], [0, 0], [0.18, 0.18], [0.26, 0.26], [0.35, 0.35]])  # rad\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        super().reset()\n",
    "        self.pid.reset()\n",
    "\n",
    "    def get_action(self, obs:np.ndarray, dt:float, **kwargs) -> np.ndarray:\n",
    "        super().get_action(obs, dt)\n",
    "        ta, v, d, eta = int(obs[0]), obs[1], obs[2], obs[3]\n",
    "        \n",
    "        u = self.pid.step(eta, dt)\n",
    "        action = np.array(np.abs(self.X_m[:, ta] - u).argmin())\n",
    "        return action\n",
    "    \n",
    "\n",
    "# setting\n",
    "env = TimeLimit(PositionControlEnv(init_fish_condition=([0.0], [0.0], [0.0]), init_goal_condition=([-0.25], [0.0]), sparse_reward=False), max_episode_steps=60)\n",
    "print(f\"obs_dims: {env.observation_space.shape[0]} | act_nums: {env.action_space.n}\")\n",
    "\n",
    "controllers = []\n",
    "controllers.append(PositionControlController('pid', kp=0.95, ki=0.0, kd=0.0))\n",
    "controllers.append(DiscreteDrlController('rl-continuous', './models/position_control/6ce19d08/newest/pi.pth', env.observation_space.shape[0], env.action_space.n, [256, 256], 'cuda'))\n",
    "controllers.append(DiscreteDrlController('rl-sparse', './models/position_control/ab634024/10000/pi.pth', env.observation_space.shape[0], env.action_space.n, [256, 256], 'cuda'))\n",
    "\n",
    "\n",
    "# visualization\n",
    "fig, axs = plt.subplots(1, len(controllers), figsize=(len(controllers)*4, 4))\n",
    "\n",
    "# interaction\n",
    "for i, controller in enumerate(controllers):\n",
    "    print(controller.name)\n",
    "\n",
    "    returns = []\n",
    "    for _ in range(1):\n",
    "        rollout_data, episodic_return = one_episode_rollout(env, controller, deterministic=False, verbose=0)\n",
    "        returns.append(episodic_return)\n",
    "\n",
    "        poses = np.stack(rollout_data['state'][-1]['world']['robot_pose'])\n",
    "        goal_pose = rollout_data['state'][-1]['world']['task_goal_pose']\n",
    "\n",
    "        axs[i].plot(poses[:, 0], poses[:, 1], color='blue', label='trajectory', marker='', linestyle='--')\n",
    "        print_pose(axs[i], poses[0], 0.1, 'black', 'init pose')\n",
    "        print_pose(axs[i], poses[-1], 0.1, 'red', 'final pose')\n",
    "        axs[i].plot(goal_pose[0], goal_pose[1], marker='.', color='orange', label='goal position')\n",
    "\n",
    "    print_format(axs[i], f'{controller.name}', 'X [m]', 'Y [m]', (-1.8, 1.8), (-1.8, 1.8))\n",
    "\n",
    "    print(f\"The average return of {len(returns)} episode(s) is: {np.array(returns).mean()}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./figs/pc-near-distance.svg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path Following Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathFollowingController(Controller):\n",
    "    '''[From Simulation to Reality: A Learning Framework for Fish-Like Robots to Perform Control Tasks](https://ieeexplore.ieee.org/document/9802680/)'''\n",
    "    def __init__(self, name:str, kp_1=0.6, kp_2=2.0) -> None:\n",
    "        super().__init__(name)\n",
    "        self.kp_1 = kp_1\n",
    "        self.kp_2 = kp_2\n",
    "\n",
    "        self.D = np.array([[0.14, 0.13], [0.15, 0.16], [0.15, 0.15], [0.15, 0.17], [0.15, 0.16], [0.14, 0.16], [0.16, 0.16]])\n",
    "        self.A = np.array([[-0.26, -0.41], [-0.10, -0.24], [-0.06, -0.15], [0.01, -0.04], [0.11, 0.11], [0.20, 0.13], [0.24, 0.38]])\n",
    "        self.X_m = np.array([[-0.26, -0.26], [-0.18, -0.18], [-0.09, -0.09], [0, 0], [0.18, 0.18], [0.26, 0.26], [0.35, 0.35]])  # rad\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        super().reset()\n",
    "\n",
    "    def get_action(self, obs:np.ndarray, dt:float, **kwargs) -> np.ndarray:\n",
    "        super().get_action(obs, dt)\n",
    "        ta, v, d, psi, beta = int(obs[0]), obs[1], obs[2], obs[3], obs[4]\n",
    "\n",
    "        u = np.clip(self.kp_1 * mapping_period(beta - psi, [-np.pi, np.pi]) + self.kp_2 * d, -0.5, 0.5)\n",
    "        action = np.array(np.abs(self.A[:, ta] - u).argmin())\n",
    "        return action\n",
    "    \n",
    "\n",
    "# setting\n",
    "# env = gym.make('PathFollowing-v0')\n",
    "# random_pose = [-0.25, -0.25, -np.pi/4]\n",
    "random_pose = [0.0, 0.0, 0.0]\n",
    "# random_pose = np.random.uniform([-0.25, -0.25, -np.pi/2], [0.25, 0.25, np.pi/2])\n",
    "env = TimeLimit(PathFollowingEnv(goal_path_type='eight', init_pose_condition=([random_pose[0]], [random_pose[1]], [random_pose[2]])), max_episode_steps=200)\n",
    "\n",
    "controllers = []\n",
    "controllers.append(PathFollowingController('pid-line', kp_1=1.2, kp_2=1.5))\n",
    "controllers.append(PathFollowingController('pid-eight', kp_1=0.6, kp_2=2.0))\n",
    "controllers.append(DiscreteDrlController('rl-line', './models/path_following/79959468/1265/pi.pth', env.observation_space.shape[0], env.action_space.n, [256, 256], 'cuda'))\n",
    "controllers.append(DiscreteDrlController('rl-eight', './models/path_following/fad9f62c/4700/pi.pth', env.observation_space.shape[0], env.action_space.n, [256, 256], 'cuda'))\n",
    "\n",
    "\n",
    "# visualization\n",
    "fig, axs = plt.subplots(1, len(controllers), figsize=(len(controllers)*4, 4))\n",
    "\n",
    "# interaction\n",
    "for i, controller in enumerate(controllers):\n",
    "    if controller is not None:\n",
    "        print(controller.name)\n",
    "\n",
    "    returns = []\n",
    "    for _ in range(1):\n",
    "        rollout_data, episodic_return = one_episode_rollout(env, controller, deterministic=False, verbose=0)\n",
    "        returns.append(episodic_return)\n",
    "        # if episodic_return <= 50:\n",
    "        #     break\n",
    "    print(f\"The average return of {len(returns)} episode(s) is: {round(np.array(returns).mean(), 1)}\")\n",
    "\n",
    "    poses = np.stack(rollout_data['state'][-1]['world']['robot_pose'])\n",
    "    goal_pose = rollout_data['state'][-1]['world']['task_goal_pose']\n",
    "\n",
    "    axs[i].plot(goal_pose[:, 0], goal_pose[:, 1], color='orange', label='goal path', marker='', linestyle='--')\n",
    "    axs[i].plot(poses[:, 0], poses[:, 1], color='blue', label='trajectory', marker='', linestyle='-')\n",
    "    print_pose(axs[i], poses[0], 0.1, 'black', 'init pose')\n",
    "    print_pose(axs[i], poses[-1], 0.1, 'red', 'final pose')\n",
    "    axs[i].text(1.7, 1.7, f'return:{round(np.array(returns).mean(), 1)}', horizontalalignment='right', verticalalignment='top', fontsize=12, color='red')\n",
    "\n",
    "    print_format(axs[i], f'{controller.name}', 'X [m]', 'Y [m]', (-1.8, 1.8), (-1.8, 1.8))\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# plt.savefig('./figs/pf-eight.svg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pose Regulation Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseRegulationController(Controller):\n",
    "    '''[CFD based parameter tuning for motion control of robotic fish](https://iopscience.iop.org/article/10.1088/1748-3190/ab6b6c)'''\n",
    "    def __init__(self, name:str) -> None:\n",
    "        super().__init__(name)\n",
    "        self.D = np.array([[0.14, 0.13], [0.15, 0.16], [0.15, 0.15], [0.15, 0.17], [0.15, 0.16], [0.14, 0.16], [0.16, 0.16]])\n",
    "        self.A = np.array([[-0.26, -0.41], [-0.10, -0.24], [-0.06, -0.15], [0.01, -0.04], [0.11, 0.11], [0.20, 0.13], [0.24, 0.38]])\n",
    "        self.X_m = np.array([[-0.26, -0.26], [-0.18, -0.18], [-0.09, -0.09], [0, 0], [0.18, 0.18], [0.26, 0.26], [0.35, 0.35]])  # rad\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        super().reset()\n",
    "        self.stage = 0\n",
    "\n",
    "    def get_action(self, obs:np.ndarray, dt:float, **kwargs) -> np.ndarray:\n",
    "        super().get_action(obs, dt)\n",
    "        ta, v, diff_p, diff_phi, eta = int(obs[0]), obs[1], obs[2], obs[3], obs[4]\n",
    "    \n",
    "        x_L = - np.cos(mapping_period(diff_phi - eta, [-np.pi, np.pi])) * diff_p\n",
    "        y_L = np.sin(mapping_period(diff_phi - eta, [-np.pi, np.pi])) * diff_p\n",
    "\n",
    "        if self.stage == 0:                \n",
    "            theta_aux1 = mapping_period((0.95 * np.pi) * np.sign(y_L), [-np.pi, np.pi])\n",
    "            u = 0.5 * mapping_period(- diff_phi - theta_aux1, [-np.pi, np.pi])\n",
    "\n",
    "            self.stage = 1 if x_L <= -0.9 else self.stage\n",
    "        elif self.stage == 1:\n",
    "            theta_aux2 = mapping_period(- (2/3 * np.pi) * np.sign(y_L), [-np.pi, np.pi])\n",
    "            u = 1.8 * mapping_period(- diff_phi, [-np.pi, np.pi]) + 5.5 * np.abs(y_L) * mapping_period(- diff_phi - theta_aux2, [-np.pi, np.pi])\n",
    "            \n",
    "            self.stage = 2 if np.abs(y_L) <= 0.2 else self.stage\n",
    "        elif self.stage == 2:\n",
    "            theta_aux2 = mapping_period(- (1/2 * np.pi) * np.sign(y_L), [-np.pi, np.pi])\n",
    "            u = 2.6 * mapping_period(- diff_phi, [-np.pi, np.pi]) + 9.8 * np.abs(y_L) * mapping_period(- diff_phi - theta_aux2, [-np.pi, np.pi])\n",
    "        \n",
    "        action = np.array(np.abs(self.X_m[:, ta] + u).argmin())\n",
    "        return action\n",
    "    \n",
    "\n",
    "# setting\n",
    "random_pose = [1.4, -0.99, 0.01]  # Easy-Level Evaluation Case\n",
    "random_pose = [1.5, -1.57, -0.15]  # Middle-Level Evaluation Case\n",
    "random_pose = [1.8, 1.57, 2.55]  # Hard-Level Evaluation Case\n",
    "env = TimeLimit(PoseRegulationEnv(init_goal_condition=([random_pose[0]], [random_pose[1]], [random_pose[2]])), max_episode_steps=100)\n",
    "\n",
    "controllers = []\n",
    "controllers.append(None)  # random\n",
    "controllers.append(PoseRegulationController('pid'))  # \n",
    "controllers.append(DiscreteDrlController('il-pid', './models/pose_regulation/imitation_learning/pi.pth', env.observation_space.shape[0], env.action_space.n, [256, 256], 'cuda'))\n",
    "# controllers.append(DiscreteDrlController('rl-easy', './models/pose_regulation/c67b1ae8/16200/pi.pth', env.observation_space.shape[0], env.action_space.n, [32], 'cuda'))\n",
    "# controllers.append(DiscreteDrlController('rl-middle', './models/pose_regulation/d4aa9738/23600/pi.pth', env.observation_space.shape[0], env.action_space.n, [32], 'cuda'))\n",
    "controllers.append(DiscreteDrlController('rl-hard', './models/pose_regulation/5bb24efe/32500/pi.pth', env.observation_space.shape[0], env.action_space.n, [32], 'cuda'))\n",
    "\n",
    "# visualization\n",
    "fig, axs = plt.subplots(1, len(controllers), figsize=(len(controllers)*4, 4))\n",
    "\n",
    "# interaction\n",
    "for i, controller in enumerate(controllers):\n",
    "    if controller is not None:\n",
    "        print(controller.name)\n",
    "    else:\n",
    "        print('random')\n",
    "\n",
    "    returns = []\n",
    "    for j in range(10):\n",
    "        rollout_data, episodic_return = one_episode_rollout(env, controller, deterministic=True, verbose=0)\n",
    "        returns.append(1 if episodic_return >= 0 else 0)\n",
    "\n",
    "        poses = copy.deepcopy(np.stack(rollout_data['state'][-1]['world']['robot_pose']))\n",
    "        goal_pose = copy.deepcopy(rollout_data['state'][-1]['world']['task_goal_pose'])\n",
    "\n",
    "        axs[i].plot(poses[:, 0], poses[:, 1], color='blue', label='trajectory', marker='', linestyle='--')\n",
    "        print_pose(axs[i], poses[0], 0.1, 'black', 'init pose')\n",
    "        print_pose(axs[i], poses[-1], 0.1, 'red', 'final pose')\n",
    "        print_pose(axs[i], goal_pose, 0.1, 'orange', 'goal pose')\n",
    "\n",
    "        if controller is not None:\n",
    "            print_format(axs[i], f'{controller.name}', 'X [m]', 'Y [m]', (-2.0, 2.0), (-2.0, 2.0))\n",
    "        else:\n",
    "            print_format(axs[i], 'random', 'X [m]', 'Y [m]', (-2.0, 2.0), (-2.0, 2.0))\n",
    "    \n",
    "    print(f\"The successful rate of {len(returns)} episode(s) is: {np.array(returns).mean()}\")\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# plt.savefig('./figs/pr-hard.svg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imitation Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-params\n",
    "env_name = 'PoseRegulationHard-v0'\n",
    "max_episodes = 1e5\n",
    "model_save_interval = 100\n",
    "\n",
    "# setting\n",
    "env = gym.make(env_name)\n",
    "expert = PoseRegulationController('pid')  # based on 'PID' controller\n",
    "\n",
    "# logger\n",
    "uid = str(uuid.uuid1()).split('-')[0]\n",
    "logger = SummaryWriter(log_dir=f\"./tensorboard/{env_name}/il-{expert.name}/{uid}\")\n",
    "\n",
    "# imitation learning\n",
    "pi_net = DiscretePolicyNetwork(env.observation_space.shape[0], env.action_space.n, [256, 256]).to(torch.device('cuda'))\n",
    "pi_net.load_state_dict(torch.load('./checkpoints/5a9a6926/10000/pi.pth'))\n",
    "pi_net_optim = torch.optim.Adam(pi_net.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# interaction\n",
    "episodes, returns = 10000, []\n",
    "while episodes <= max_episodes:\n",
    "    rollout_data, _ = one_episode_rollout(env, expert, verbose=0)\n",
    "    \n",
    "    obs, act = rollout_data['obs'], rollout_data['act']\n",
    "    \n",
    "    obs = torch.FloatTensor(obs).to(torch.device('cuda'))\n",
    "    act = torch.LongTensor(act).to(torch.device('cuda'))\n",
    "\n",
    "    # train\n",
    "    pred_act = pi_net(obs)\n",
    "    loss = torch.nn.NLLLoss()(torch.log(pred_act), act)  # cross entropy loss\n",
    "\n",
    "    pi_net_optim.zero_grad()\n",
    "    loss.backward()\n",
    "    pi_net_optim.step()\n",
    "\n",
    "    episodes += 1\n",
    "\n",
    "    # eval\n",
    "    if not os.path.exists(f\"./checkpoints/{uid}/\"):\n",
    "        os.makedirs(f\"./checkpoints/{uid}/\")\n",
    "    torch.save(pi_net.state_dict(), f\"./checkpoints/{uid}/temp_pi.pth\")\n",
    "    student = DiscreteDrlController('il-pid', f\"./checkpoints/{uid}/temp_pi.pth\", env.observation_space.shape[0], env.action_space.n, [256, 256], 'cuda')\n",
    "    _, episode_rew = one_episode_rollout(env, student, verbose=0)\n",
    "\n",
    "    returns.append(episode_rew)\n",
    "    average_return = np.array(returns).mean() if len(returns) <= 50 else np.array(returns[-51:-1]).mean()\n",
    "\n",
    "    # verbose\n",
    "    print(f\"{uid} | Episodes: {episodes} | Episode Reward: {round(episode_rew, 5)} | Average Return: {round(average_return, 5)} | Loss: {loss.item()}\")\n",
    "\n",
    "    logger.add_scalar('episodic/return', episode_rew, episodes)\n",
    "    logger.add_scalar('episodic/return(average)', average_return, episodes)\n",
    "    logger.add_scalar('loss/pi', loss, episodes)\n",
    "\n",
    "    # save model\n",
    "    if episodes % model_save_interval == 0:\n",
    "        if not os.path.exists(f\"./checkpoints/{uid}/{episodes}/\"):\n",
    "            os.makedirs(f\"./checkpoints/{uid}/{episodes}/\")\n",
    "        torch.save(pi_net.state_dict(), f\"./checkpoints/{uid}/{episodes}/pi.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oceanus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
